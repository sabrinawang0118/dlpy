{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/dlpy/transformers/bert_model.py:45: UserWarning: You are using a version of the transformers package (4.0.0) that has not been tested for compatibility.  Unexpected behavior may occur.\n",
      "  warnings.warn(warn_message,UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: loading base model bert-base-uncased ...\n",
      "NOTE: base model bert-base-uncased loaded.\n",
      "NOTE: creating embedding table ... \n",
      "NOTE: embedding table created and loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: 10% of the observations tokenized.\n",
      "NOTE: 20% of the observations tokenized.\n",
      "NOTE: 30% of the observations tokenized.\n",
      "NOTE: 40% of the observations tokenized.\n",
      "NOTE: 50% of the observations tokenized.\n",
      "NOTE: 60% of the observations tokenized.\n",
      "NOTE: 70% of the observations tokenized.\n",
      "NOTE: 80% of the observations tokenized.\n",
      "NOTE: 90% of the observations tokenized.\n",
      "NOTE: 100% of the observations tokenized.\n",
      "NOTE: all observations tokenized.\n",
      "\n",
      "NOTE: uploading training data to table bert_train_data.\n",
      "NOTE: there are 1444 observations in the training data set.\n",
      "\n",
      "NOTE: uploading test/validation data to table bert_test_validation_data.\n",
      "NOTE: there are 394 observations in the test/validation data set.\n",
      "\n",
      "NOTE: training and test/validation data sets ready.\n",
      "\n",
      "NOTE: HDF5 file is /home/sasdemo/sentiment_analysis/cache_dir1/bert-base-uncased.kerasmodel.h5\n",
      "NOTE: Model compiled successfully.\n",
      "NOTE: Model weights attached successfully!\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 86828546.\n",
      "NOTE:  The approximate memory cost is 11728.00 MB.\n",
      "NOTE:  Loading weights cost       2.90 (s).\n",
      "NOTE:  Initializing each layer cost      12.42 (s).\n",
      "NOTE:  The total number of threads on each worker is 12.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 1.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 12.\n",
      "NOTE:  Target variable: _target_0_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: 1\n",
      "NOTE:  Level      1: 2\n",
      "NOTE:  Number of input variables:     3\n",
      "NOTE:  Number of text input variables:      3\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    12  0.00002           0.6254     0.3333    54.06\n",
      "NOTE:      1    12  0.00002           0.4509    0.08333    39.37\n",
      "NOTE:      2    12  0.00002           0.3398          0    79.85\n",
      "NOTE:      3    12  0.00002           0.2917          0   101.74\n",
      "NOTE:      4    12  0.00002           0.2676          0    63.67\n",
      "NOTE:      5    12  0.00002           0.4386     0.1667   115.13\n",
      "NOTE:      6    12  0.00002           0.4573     0.1667   132.67\n",
      "NOTE:      7    12  0.00002           0.8164     0.4167    66.10\n",
      "NOTE:      8    12  0.00002           0.3143    0.08333    82.07\n",
      "NOTE:      9    12  0.00002           0.7015     0.3333    55.48\n",
      "NOTE:     10    12  0.00002           0.4402     0.1667    24.57\n",
      "NOTE:     11    12  0.00002           0.4364     0.1667    42.03\n",
      "NOTE:     12    12  0.00002           0.3083    0.08333    35.45\n",
      "NOTE:     13    12  0.00002           0.5574       0.25    33.61\n",
      "NOTE:     14    12  0.00002           0.2961    0.08333    35.64\n",
      "NOTE:     15    12  0.00002           0.3005    0.08333    35.88\n",
      "NOTE:     16    12  0.00002           0.3098    0.08333    59.45\n",
      "NOTE:     17    12  0.00002           0.4168     0.1667   131.00\n",
      "NOTE:     18    12  0.00002           0.2875    0.08333    60.94\n",
      "NOTE:     19    12  0.00002           0.2669    0.08333    85.24\n",
      "NOTE:     20    12  0.00002           0.9764        0.5   159.31\n",
      "NOTE:     21    12  0.00002            0.131          0    88.39\n",
      "NOTE:     22    12  0.00002           0.1278          0    42.52\n",
      "NOTE:     23    12  0.00002           0.7137     0.3333    50.78\n",
      "NOTE:     24    12  0.00002           0.5726       0.25    39.66\n",
      "NOTE:     25    12  0.00002           0.4411     0.1667    49.06\n",
      "NOTE:     26    12  0.00002           0.7437     0.4167   100.21\n",
      "NOTE:     27    12  0.00002           0.4975       0.25    48.25\n",
      "NOTE:     28    12  0.00002           0.3283    0.08333    62.77\n",
      "NOTE:     29    12  0.00002           0.5181       0.25    49.35\n",
      "NOTE:     30    12  0.00002           0.4844       0.25    96.17\n",
      "NOTE:     31    12  0.00002           0.2124          0    42.95\n",
      "NOTE:     32    12  0.00002           0.7273     0.4167    72.84\n",
      "NOTE:     33    12  0.00002           0.3693     0.1667    45.40\n",
      "NOTE:     34    12  0.00002           0.2533    0.08333    33.82\n",
      "NOTE:     35    12  0.00002           0.2685    0.08333    32.74\n",
      "NOTE:     36    12  0.00002           0.2937    0.08333    61.55\n",
      "NOTE:     37    12  0.00002            0.325     0.1667    45.66\n",
      "NOTE:     38    12  0.00002           0.3582     0.1667    33.86\n",
      "NOTE:     39    12  0.00002           0.1833    0.08333    40.97\n",
      "NOTE:     40    12  0.00002           0.1147          0    35.82\n",
      "NOTE:     41    12  0.00002           0.5423       0.25    51.12\n",
      "NOTE:     42    12  0.00002           0.3868     0.1667    47.26\n",
      "NOTE:     43    12  0.00002           0.3786     0.1667    76.93\n",
      "NOTE:     44    12  0.00002           0.1639    0.08333    40.09\n",
      "NOTE:     45    12  0.00002           0.1743    0.08333    33.66\n",
      "NOTE:     46    12  0.00002           0.4462     0.3333    47.34\n",
      "NOTE:     47    12  0.00002           0.1885          0    48.75\n",
      "NOTE:     48    12  0.00002           0.2175          0    40.68\n",
      "NOTE:     49    12  0.00002           0.2786    0.08333    37.21\n",
      "NOTE:     50    12  0.00002           0.1986    0.08333    38.80\n",
      "NOTE:     51    12  0.00002           0.2695     0.1667    38.09\n",
      "NOTE:     52    12  0.00002           0.3705       0.25    36.03\n",
      "NOTE:     53    12  0.00002           0.3507     0.1667    27.55\n",
      "NOTE:     54    12  0.00002           0.2934    0.08333    46.74\n",
      "NOTE:     55    12  0.00002          0.09536          0    59.60\n",
      "NOTE:     56    12  0.00002            0.283    0.08333   172.51\n",
      "NOTE:     57    12  0.00002           0.1517     0.1667   155.64\n",
      "NOTE:     58    12  0.00002           0.3555     0.1667   109.03\n",
      "NOTE:     59    12  0.00002          0.08138          0    73.35\n",
      "NOTE:     60    12  0.00002           0.2325     0.1667    83.71\n",
      "NOTE:     61    12  0.00002           0.1387    0.08333    37.46\n",
      "NOTE:     62    12  0.00002           0.1879    0.08333    47.95\n",
      "NOTE:     63    12  0.00002          0.03779          0    70.66\n",
      "NOTE:     64    12  0.00002           0.1486          0    38.38\n",
      "NOTE:     65    12  0.00002           0.2902    0.08333    33.13\n",
      "NOTE:     66    12  0.00002           0.1187          0    44.89\n",
      "NOTE:     67    12  0.00002           0.2112    0.08333    44.68\n",
      "NOTE:     68    12  0.00002            0.106    0.08333    57.72\n",
      "NOTE:     69    12  0.00002          0.06802          0    41.43\n",
      "NOTE:     70    12  0.00002           0.1209    0.08333    32.71\n",
      "NOTE:     71    12  0.00002          0.07782          0    38.71\n",
      "NOTE:     72    12  0.00002          0.08122          0    40.82\n",
      "NOTE:     73    12  0.00002           0.4313     0.1667    38.90\n",
      "NOTE:     74    12  0.00002          0.07523          0    61.61\n",
      "NOTE:     75    12  0.00002          0.01561          0    41.08\n",
      "NOTE:     76    12  0.00002          0.08565          0    51.12\n",
      "NOTE:     77    12  0.00002          0.04554          0    65.92\n",
      "NOTE:     78    12  0.00002           0.5869       0.25    62.77\n",
      "NOTE:     79    12  0.00002           0.2063    0.08333    43.83\n",
      "NOTE:     80    12  0.00002           0.1729    0.08333    61.58\n",
      "NOTE:     81    12  0.00002           0.1528    0.08333    34.58\n",
      "NOTE:     82    12  0.00002           0.2075    0.08333    39.52\n",
      "NOTE:     83    12  0.00002           0.1402          0    45.57\n",
      "NOTE:     84    12  0.00002           0.1851    0.08333    41.49\n",
      "NOTE:     85    12  0.00002          0.02141          0    74.59\n",
      "NOTE:     86    12  0.00002           0.2687    0.08333    79.93\n",
      "NOTE:     87    12  0.00002          0.02616          0    73.84\n",
      "NOTE:     88    12  0.00002           0.3026    0.08333    67.51\n",
      "NOTE:     89    12  0.00002          0.05384          0    33.27\n",
      "NOTE:     90    12  0.00002          0.07234          0    58.04\n",
      "NOTE:     91    12  0.00002          0.05987          0    34.74\n",
      "NOTE:     92    12  0.00002          0.03984          0    31.93\n",
      "NOTE:     93    12  0.00002          0.07176          0    55.77\n",
      "NOTE:     94    12  0.00002          0.03397          0    54.18\n",
      "NOTE:     95    12  0.00002          0.03023          0    29.69\n",
      "NOTE:     96    12  0.00002          0.06862          0    80.64\n",
      "NOTE:     97    12  0.00002          0.02927          0    49.45\n",
      "NOTE:     98    12  0.00002          0.03194          0    39.18\n",
      "NOTE:     99    12  0.00002           0.1213          0    58.00\n",
      "NOTE:    100    12  0.00002          0.07652    0.08333    62.04\n",
      "NOTE:    101    12  0.00002          0.03818          0    53.99\n",
      "NOTE:    102    12  0.00002          0.02976          0    42.84\n",
      "NOTE:    103    12  0.00002           0.0102          0    69.32\n",
      "NOTE:    104    12  0.00002          0.01951          0    52.33\n",
      "NOTE:    105    12  0.00002          0.04739          0    53.17\n",
      "NOTE:    106    12  0.00002           0.2847    0.08333    68.99\n",
      "NOTE:    107    12  0.00002           0.1186    0.08333    64.62\n",
      "NOTE:    108    12  0.00002          0.09153    0.08333    40.93\n",
      "NOTE:    109    12  0.00002          0.02408          0    39.68\n",
      "NOTE:    110    12  0.00002            0.579     0.1667    40.93\n",
      "NOTE:    111    12  0.00002          0.03135          0    37.53\n",
      "NOTE:    112    12  0.00002          0.03441          0    29.77\n",
      "NOTE:    113    12  0.00002           0.1954    0.08333    70.67\n",
      "NOTE:    114    12  0.00002          0.09098    0.08333    36.55\n",
      "NOTE:    115    12  0.00002          0.02055          0    34.35\n",
      "NOTE:    116    12  0.00002           0.3139    0.08333    38.31\n",
      "NOTE:    117    12  0.00002           0.3118     0.1667    48.49\n",
      "NOTE:    118    12  0.00002           0.1312    0.08333    55.48\n",
      "NOTE:    119    12  0.00002           0.2367    0.08333    58.15\n",
      "NOTE:    120    12  0.00002           0.1722     0.1667    37.80\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          2E-5          0.2513     0.0978  6803.77\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    12  0.00002          0.03084          0    36.93\n",
      "NOTE:      1    12  0.00002           0.1029          0    74.07\n",
      "NOTE:      2    12  0.00002           0.1108          0    51.85\n",
      "NOTE:      3    12  0.00002          0.05967          0    41.40\n",
      "NOTE:      4    12  0.00002          0.08388          0    75.80\n",
      "NOTE:      5    12  0.00002           0.3105    0.08333    45.58\n",
      "NOTE:      6    12  0.00002           0.1464    0.08333    50.47\n",
      "NOTE:      7    12  0.00002          0.04387          0    39.65\n",
      "NOTE:      8    12  0.00002          0.02218          0    39.87\n",
      "NOTE:      9    12  0.00002           0.0512          0    83.34\n",
      "NOTE:     10    12  0.00002          0.03559          0    39.07\n",
      "NOTE:     11    12  0.00002           0.1416    0.08333    51.97\n",
      "NOTE:     12    12  0.00002           0.5015     0.1667    40.73\n",
      "NOTE:     13    12  0.00002          0.02943          0    63.04\n",
      "NOTE:     14    12  0.00002          0.03602          0    38.32\n",
      "NOTE:     15    12  0.00002          0.03092          0    59.78\n",
      "NOTE:     16    12  0.00002          0.03184          0    61.01\n",
      "NOTE:     17    12  0.00002          0.02307          0    31.63\n",
      "NOTE:     18    12  0.00002           0.2357    0.08333    38.47\n",
      "NOTE:     19    12  0.00002             0.18    0.08333    41.59\n",
      "NOTE:     20    12  0.00002            0.167    0.08333    51.78\n",
      "NOTE:     21    12  0.00002           0.1564    0.08333    53.22\n",
      "NOTE:     22    12  0.00002          0.03342          0    65.95\n",
      "NOTE:     23    12  0.00002          0.05947          0    35.68\n",
      "NOTE:     24    12  0.00002          0.01917          0    34.41\n",
      "NOTE:     25    12  0.00002           0.4022    0.08333    40.89\n",
      "NOTE:     26    12  0.00002           0.1013    0.08333    55.93\n",
      "NOTE:     27    12  0.00002          0.01413          0    66.00\n",
      "NOTE:     28    12  0.00002          0.01627          0    41.91\n",
      "NOTE:     29    12  0.00002           0.1563    0.08333    64.91\n",
      "NOTE:     30    12  0.00002           0.3457    0.08333    44.33\n",
      "NOTE:     31    12  0.00002          0.01779          0    35.74\n",
      "NOTE:     32    12  0.00002           0.1379    0.08333    44.93\n",
      "NOTE:     33    12  0.00002          0.01138          0    31.01\n",
      "NOTE:     34    12  0.00002          0.05189          0    42.72\n",
      "NOTE:     35    12  0.00002          0.07424          0    71.74\n",
      "NOTE:     36    12  0.00002          0.05232          0    45.62\n",
      "NOTE:     37    12  0.00002          0.08522          0    69.77\n",
      "NOTE:     38    12  0.00002          0.02669          0    40.44\n",
      "NOTE:     39    12  0.00002           0.2866    0.08333    38.99\n",
      "NOTE:     40    12  0.00002           0.0436          0    33.71\n",
      "NOTE:     41    12  0.00002           0.3097    0.08333    36.88\n",
      "NOTE:     42    12  0.00002           0.1625    0.08333    96.09\n",
      "NOTE:     43    12  0.00002          0.04666          0    51.23\n",
      "NOTE:     44    12  0.00002           0.1037          0   103.27\n",
      "NOTE:     45    12  0.00002            0.145    0.08333    56.88\n",
      "NOTE:     46    12  0.00002           0.1576    0.08333    83.82\n",
      "NOTE:     47    12  0.00002           0.2309    0.08333    64.73\n",
      "NOTE:     48    12  0.00002           0.8338       0.25    45.74\n",
      "NOTE:     49    12  0.00002          0.01591          0    37.29\n",
      "NOTE:     50    12  0.00002          0.03713          0    32.72\n",
      "NOTE:     51    12  0.00002          0.01542          0    56.29\n",
      "NOTE:     52    12  0.00002          0.03327          0    83.42\n",
      "NOTE:     53    12  0.00002          0.05199          0    76.90\n",
      "NOTE:     54    12  0.00002           0.0757          0    35.20\n",
      "NOTE:     55    12  0.00002          0.02956          0    41.87\n",
      "NOTE:     56    12  0.00002           0.4339     0.1667    67.96\n",
      "NOTE:     57    12  0.00002           0.3329     0.1667    47.61\n",
      "NOTE:     58    12  0.00002          0.01827          0    63.00\n",
      "NOTE:     59    12  0.00002          0.02037          0    37.19\n",
      "NOTE:     60    12  0.00002          0.07501    0.08333    33.18\n",
      "NOTE:     61    12  0.00002          0.01505          0    46.17\n",
      "NOTE:     62    12  0.00002           0.0591          0    30.08\n",
      "NOTE:     63    12  0.00002          0.05599          0    80.50\n",
      "NOTE:     64    12  0.00002          0.02439          0    29.90\n",
      "NOTE:     65    12  0.00002           0.2963    0.08333    53.46\n",
      "NOTE:     66    12  0.00002          0.03001          0    34.08\n",
      "NOTE:     67    12  0.00002          0.09469          0    54.87\n",
      "NOTE:     68    12  0.00002           0.1952    0.08333    41.22\n",
      "NOTE:     69    12  0.00002          0.04527          0    60.35\n",
      "NOTE:     70    12  0.00002          0.05147          0    41.91\n",
      "NOTE:     71    12  0.00002           0.2568    0.08333    38.11\n",
      "NOTE:     72    12  0.00002          0.04521          0    28.13\n",
      "NOTE:     73    12  0.00002            0.114    0.08333    34.06\n",
      "NOTE:     74    12  0.00002           0.3481     0.1667    42.53\n",
      "NOTE:     75    12  0.00002          0.01969          0    26.72\n",
      "NOTE:     76    12  0.00002          0.01141          0    46.37\n",
      "NOTE:     77    12  0.00002         0.009733          0    44.27\n",
      "NOTE:     78    12  0.00002          0.07272          0    42.07\n",
      "NOTE:     79    12  0.00002          0.01373          0    41.97\n",
      "NOTE:     80    12  0.00002         0.008198          0    66.44\n",
      "NOTE:     81    12  0.00002           0.0131          0    48.88\n",
      "NOTE:     82    12  0.00002           0.3279    0.08333    54.84\n",
      "NOTE:     83    12  0.00002          0.02552          0    46.19\n",
      "NOTE:     84    12  0.00002         0.009771          0    40.80\n",
      "NOTE:     85    12  0.00002          0.01949          0    94.96\n",
      "NOTE:     86    12  0.00002          0.03494          0    82.85\n",
      "NOTE:     87    12  0.00002          0.03418          0    69.88\n",
      "NOTE:     88    12  0.00002          0.03204          0    95.35\n",
      "NOTE:     89    12  0.00002           0.0128          0    77.46\n",
      "NOTE:     90    12  0.00002           0.1944    0.08333    70.56\n",
      "NOTE:     91    12  0.00002          0.01915          0    44.08\n",
      "NOTE:     92    12  0.00002          0.05332          0    49.67\n",
      "NOTE:     93    12  0.00002         0.006443          0    50.66\n",
      "NOTE:     94    12  0.00002          0.03262          0    43.50\n",
      "NOTE:     95    12  0.00002          0.04432          0    66.81\n",
      "NOTE:     96    12  0.00002          0.02573          0    51.31\n",
      "NOTE:     97    12  0.00002          0.01444          0    38.31\n",
      "NOTE:     98    12  0.00002          0.00844          0    38.35\n",
      "NOTE:     99    12  0.00002          0.02624          0    53.51\n",
      "NOTE:    100    12  0.00002          0.02239          0    46.11\n",
      "NOTE:    101    12  0.00002           0.0123          0    33.67\n",
      "NOTE:    102    12  0.00002           0.3541    0.08333    78.72\n",
      "NOTE:    103    12  0.00002          0.01132          0    34.91\n",
      "NOTE:    104    12  0.00002           0.2987    0.08333    56.39\n",
      "NOTE:    105    12  0.00002          0.01683          0    84.60\n",
      "NOTE:    106    12  0.00002           0.0343          0    46.46\n",
      "NOTE:    107    12  0.00002          0.01764          0    38.93\n",
      "NOTE:    108    12  0.00002          0.02161          0    37.11\n",
      "NOTE:    109    12  0.00002           0.2117    0.08333    50.90\n",
      "NOTE:    110    12  0.00002           0.2341    0.08333    36.43\n",
      "NOTE:    111    12  0.00002           0.0117          0    52.23\n",
      "NOTE:    112    12  0.00002         0.007962          0    35.50\n",
      "NOTE:    113    12  0.00002          0.01099          0    59.20\n",
      "NOTE:    114    12  0.00002         0.006443          0    46.37\n",
      "NOTE:    115    12  0.00002         0.006658          0    37.16\n",
      "NOTE:    116    12  0.00002           0.0424          0    40.85\n",
      "NOTE:    117    12  0.00002          0.01088          0    62.24\n",
      "NOTE:    118    12  0.00002          0.01012          0    29.09\n",
      "NOTE:    119    12  0.00002          0.01979          0    55.79\n",
      "NOTE:    120    12  0.00002            0.105    0.08333    53.70\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          2E-5         0.09723    0.02824  6183.50\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error   Time(s) (Training)\n",
      "NOTE:      0    12  0.00002          0.01337          0    39.44\n",
      "NOTE:      1    12  0.00002           0.1304    0.08333    51.90\n",
      "NOTE:      2    12  0.00002          0.01535          0    42.95\n",
      "NOTE:      3    12  0.00002          0.01015          0    62.11\n",
      "NOTE:      4    12  0.00002         0.007359          0    68.61\n",
      "NOTE:      5    12  0.00002         0.003354          0    31.82\n",
      "NOTE:      6    12  0.00002         0.007082          0    77.54\n",
      "NOTE:      7    12  0.00002         0.008797          0    52.92\n",
      "NOTE:      8    12  0.00002          0.03296          0    70.38\n",
      "NOTE:      9    12  0.00002         0.008684          0    44.52\n",
      "NOTE:     10    12  0.00002           0.0322          0   111.11\n",
      "NOTE:     11    12  0.00002          0.01811          0    31.72\n",
      "NOTE:     12    12  0.00002           0.3754    0.08333    70.41\n",
      "NOTE:     13    12  0.00002           0.1229    0.08333    67.84\n",
      "NOTE:     14    12  0.00002         0.002764          0    26.45\n",
      "NOTE:     15    12  0.00002          0.00374          0    38.79\n",
      "NOTE:     16    12  0.00002          0.05285          0    59.75\n",
      "NOTE:     17    12  0.00002          0.01171          0    74.88\n",
      "NOTE:     18    12  0.00002           0.0101          0    36.01\n",
      "NOTE:     19    12  0.00002          0.01051          0    24.89\n",
      "NOTE:     20    12  0.00002         0.005709          0    39.71\n",
      "NOTE:     21    12  0.00002          0.02541          0    79.54\n",
      "NOTE:     22    12  0.00002          0.00774          0    28.13\n",
      "NOTE:     23    12  0.00002          0.02884          0    66.64\n",
      "NOTE:     24    12  0.00002           0.3082    0.08333    36.50\n",
      "NOTE:     25    12  0.00002          0.02101          0    81.49\n",
      "NOTE:     26    12  0.00002          0.02282          0    80.24\n",
      "NOTE:     27    12  0.00002          0.03432          0    54.18\n",
      "NOTE:     28    12  0.00002          0.01869          0    37.77\n",
      "NOTE:     29    12  0.00002          0.04663          0    72.96\n",
      "NOTE:     30    12  0.00002           0.0525          0    33.41\n",
      "NOTE:     31    12  0.00002          0.02111          0    42.21\n",
      "NOTE:     32    12  0.00002           0.0139          0    57.74\n",
      "NOTE:     33    12  0.00002          0.08727    0.08333    54.79\n",
      "NOTE:     34    12  0.00002         0.005194          0    66.91\n",
      "NOTE:     35    12  0.00002          0.01386          0    43.69\n",
      "NOTE:     36    12  0.00002         0.006579          0    51.36\n",
      "NOTE:     37    12  0.00002         0.005735          0   112.44\n",
      "NOTE:     38    12  0.00002         0.009564          0   100.94\n",
      "NOTE:     39    12  0.00002         0.006202          0    37.14\n",
      "NOTE:     40    12  0.00002         0.005936          0    25.76\n",
      "NOTE:     41    12  0.00002           0.2425    0.08333    61.71\n",
      "NOTE:     42    12  0.00002          0.04564          0    86.91\n",
      "NOTE:     43    12  0.00002         0.005758          0   114.87\n",
      "NOTE:     44    12  0.00002          0.01145          0    76.39\n",
      "NOTE:     45    12  0.00002          0.01081          0    38.18\n",
      "NOTE:     46    12  0.00002          0.07288    0.08333    46.29\n",
      "NOTE:     47    12  0.00002         0.002183          0   131.28\n",
      "NOTE:     48    12  0.00002         0.005637          0    32.97\n",
      "NOTE:     49    12  0.00002           0.2094    0.08333   192.93\n",
      "NOTE:     50    12  0.00002          0.07259    0.08333    90.84\n",
      "NOTE:     51    12  0.00002          0.06292          0   106.79\n",
      "NOTE:     52    12  0.00002         0.002819          0    75.08\n",
      "NOTE:     53    12  0.00002         0.006271          0    80.64\n",
      "NOTE:     54    12  0.00002         0.003514          0    31.76\n",
      "NOTE:     55    12  0.00002          0.09701    0.08333    49.39\n",
      "NOTE:     56    12  0.00002          0.05429          0    51.44\n",
      "NOTE:     57    12  0.00002         0.005705          0    33.53\n",
      "NOTE:     58    12  0.00002          0.01317          0    31.19\n",
      "NOTE:     59    12  0.00002            0.147    0.08333    61.17\n",
      "NOTE:     60    12  0.00002         0.002985          0    37.82\n",
      "NOTE:     61    12  0.00002          0.06079          0    92.67\n",
      "NOTE:     62    12  0.00002          0.02704          0    93.54\n",
      "NOTE:     63    12  0.00002          0.07563          0   131.59\n",
      "NOTE:     64    12  0.00002          0.03654          0    69.30\n",
      "NOTE:     65    12  0.00002          0.01409          0    65.10\n",
      "NOTE:     66    12  0.00002           0.1592    0.08333    41.14\n",
      "NOTE:     67    12  0.00002           0.0101          0    41.60\n",
      "NOTE:     68    12  0.00002          0.08588    0.08333    38.11\n",
      "NOTE:     69    12  0.00002          0.01642          0    79.06\n",
      "NOTE:     70    12  0.00002           0.0109          0    72.14\n",
      "NOTE:     71    12  0.00002          0.01215          0    72.24\n",
      "NOTE:     72    12  0.00002          0.09088    0.08333    76.32\n",
      "NOTE:     73    12  0.00002           0.1644    0.08333   107.95\n",
      "NOTE:     74    12  0.00002         0.005786          0    62.44\n",
      "NOTE:     75    12  0.00002         0.005578          0   101.28\n",
      "NOTE:     76    12  0.00002          0.05787          0    60.23\n",
      "NOTE:     77    12  0.00002         0.007711          0    60.58\n",
      "NOTE:     78    12  0.00002          0.01236          0    67.76\n",
      "NOTE:     79    12  0.00002         0.003604          0    36.01\n",
      "NOTE:     80    12  0.00002         0.006851          0    37.15\n",
      "NOTE:     81    12  0.00002         0.004043          0    53.90\n",
      "NOTE:     82    12  0.00002          0.01465          0    31.69\n",
      "NOTE:     83    12  0.00002         0.004014          0    90.57\n",
      "NOTE:     84    12  0.00002         0.007165          0    50.04\n",
      "NOTE:     85    12  0.00002         0.002932          0    93.03\n",
      "NOTE:     86    12  0.00002         0.006786          0    51.02\n",
      "NOTE:     87    12  0.00002         0.006868          0    39.92\n",
      "NOTE:     88    12  0.00002         0.002021          0    47.58\n",
      "NOTE:     89    12  0.00002         0.002027          0    60.47\n",
      "NOTE:     90    12  0.00002          0.00262          0    51.51\n",
      "NOTE:     91    12  0.00002         0.003169          0    81.79\n",
      "NOTE:     92    12  0.00002         0.002545          0    56.68\n",
      "NOTE:     93    12  0.00002         0.001909          0    41.70\n",
      "NOTE:     94    12  0.00002         0.003628          0    70.85\n",
      "NOTE:     95    12  0.00002         0.005164          0    30.77\n",
      "NOTE:     96    12  0.00002           0.4578    0.08333    60.40\n",
      "NOTE:     97    12  0.00002         0.002644          0    44.25\n",
      "NOTE:     98    12  0.00002         0.004315          0    72.62\n",
      "NOTE:     99    12  0.00002         0.002771          0    46.36\n",
      "NOTE:    100    12  0.00002         0.002125          0    73.79\n",
      "NOTE:    101    12  0.00002          0.01225          0    55.42\n",
      "NOTE:    102    12  0.00002         0.004424          0    74.79\n",
      "NOTE:    103    12  0.00002         0.008674          0    96.66\n",
      "NOTE:    104    12  0.00002          0.01138          0    40.57\n",
      "NOTE:    105    12  0.00002         0.002567          0    56.07\n",
      "NOTE:    106    12  0.00002         0.002679          0    42.40\n",
      "NOTE:    107    12  0.00002         0.002923          0    58.20\n",
      "NOTE:    108    12  0.00002          0.01279          0    47.39\n",
      "NOTE:    109    12  0.00002         0.003833          0    38.06\n",
      "NOTE:    110    12  0.00002         0.005268          0    46.06\n",
      "NOTE:    111    12  0.00002         0.003885          0    35.41\n",
      "NOTE:    112    12  0.00002         0.004577          0    54.69\n",
      "NOTE:    113    12  0.00002         0.001945          0    38.21\n",
      "NOTE:    114    12  0.00002          0.08345    0.08333    62.24\n",
      "NOTE:    115    12  0.00002         0.001935          0    41.81\n",
      "NOTE:    116    12  0.00002         0.006073          0    55.50\n",
      "NOTE:    117    12  0.00002         0.004758          0    58.03\n",
      "NOTE:    118    12  0.00002         0.004804          0    38.80\n",
      "NOTE:    119    12  0.00002           0.1068    0.08333    40.97\n",
      "NOTE:    120    12  0.00002         0.003538          0    43.47\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          2E-5         0.03629     0.0124  7299.07\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is   20286.34 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Recurrent Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Recurrent Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of multi-head attention layers</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of layer normalization layers</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>86724864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>103682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>86828546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>11757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.251275</td>\n",
       "      <td>0.097796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.097232</td>\n",
       "      <td>0.028237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>0.012397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(sasdemo)</td>\n",
       "      <td>classification_weights</td>\n",
       "      <td>86828546</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('classification_weights', caslib='CAS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 2.03e+04s</span> &#183; <span class=\"cas-user\">user 1.02e+05s</span> &#183; <span class=\"cas-sys\">sys 85.4s</span> &#183; <span class=\"cas-memory\">mem 1.21e+04MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                     Value\n",
       " 0                                  Model Name            classification\n",
       " 1                                  Model Type  Recurrent Neural Network\n",
       " 2                            Number of Layers                        92\n",
       " 3                      Number of Input Layers                         3\n",
       " 4                     Number of Output Layers                         1\n",
       " 5              Number of Convolutional Layers                         0\n",
       " 6                    Number of Pooling Layers                         0\n",
       " 7            Number of Fully Connected Layers                        25\n",
       " 8                  Number of Recurrent Layers                         1\n",
       " 9                   Number of Residual Layers                        25\n",
       " 10      Number of multi-head attention layers                        12\n",
       " 11       Number of layer normalization layers                        25\n",
       " 12                Number of Weight Parameters                  86724864\n",
       " 13                  Number of Bias Parameters                    103682\n",
       " 14           Total Number of Model Parameters                  86828546\n",
       " 15  Approximate Memory Cost for Training (MB)                     11757\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "    Epoch  LearningRate      Loss  FitError\n",
       " 0      1       0.00002  0.251275  0.097796\n",
       " 1      2       0.00002  0.097232  0.028237\n",
       " 2      3       0.00002  0.036290  0.012397\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "              casLib                    Name      Rows  Columns  \\\n",
       " 0  CASUSER(sasdemo)  classification_weights  86828546        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('classification_weights', caslib='CAS...  \n",
       "\n",
       "+ Elapsed: 2.03e+04s, user: 1.02e+05s, sys: 85.4s, mem: 1.21e+04mb"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dlpy.transformers.bert_utils import bert_prepare_data\n",
    "from dlpy.transformers import bert_model\n",
    "import swat\n",
    "\n",
    "viya_conn = swat.CAS('10.96.1.152', 5570, 'sasdemo', 'Orion123')\n",
    "\n",
    "\n",
    "cache_dir = '/home/sasdemo/sentiment_analysis/cache_dir1'\n",
    "bert = bert_model.BERT_Model(viya_conn,\n",
    "                             cache_dir,\n",
    "                             'bert-base-uncased',\n",
    "                             2,\n",
    "                             num_hidden_layers=12,\n",
    "                             max_seq_len=256,\n",
    "                             verbose=True)\n",
    "\n",
    "reviews = pd.read_csv(\n",
    "    '/home/sasdemo/sentiment_analysis/data/Reviews.csv', header=0, encoding='utf-8')\n",
    "reviews = reviews.head(2000)\n",
    "\n",
    "t_idx = reviews[\"Score\"] != 3\n",
    "\n",
    "inputs = reviews[t_idx][\"Text\"].to_list()\n",
    "targets = reviews[t_idx][\"Score\"].to_list()\n",
    "for ii, val in enumerate(targets):\n",
    "    inputs[ii] = inputs[ii].replace(\"<br />\", \"\")\n",
    "    if (val == 1) or (val == 2):  # negative reviews\n",
    "        targets[ii] = 1\n",
    "    elif (val == 4) or (val == 5):  # positive reviews\n",
    "        targets[ii] = 2\n",
    "\n",
    "\n",
    "# the arguments max_seq_len and trucation were\n",
    "num_tgt_var, train, test = bert_prepare_data(viya_conn,\n",
    "                                             bert.get_tokenizer(),\n",
    "                                             input_a=inputs,\n",
    "                                             target=targets,\n",
    "                                             train_fraction=0.8,\n",
    "                                             segment_vocab_size=bert.get_segment_size(),\n",
    "                                             classification_problem=bert.get_problem_type(),\n",
    "                                             #  truncation=True,\n",
    "                                             max_seq_len=512,\n",
    "                                             verbose=True)\n",
    "\n",
    "bert.compile(num_target_var=num_tgt_var)\n",
    "\n",
    "bert.load_weights('/home/sasdemo/sentiment_analysis/cache_dir1/bert-base-uncased.kerasmodel.h5',\n",
    "                  num_target_var=num_tgt_var, freeze_base_model=False)\n",
    "\n",
    "\n",
    "bert.set_optimizer_parameters(learning_rate=2e-5)\n",
    "bert.fit(train,\n",
    "         data_specs=bert.get_data_spec(num_tgt_var),\n",
    "         optimizer=bert.get_optimizer(),\n",
    "         text_parms=bert.get_text_parameters(),\n",
    "         seed=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Descr         Value\n",
      "0  Number of Observations Read           394\n",
      "1  Number of Observations Used           394\n",
      "2  Misclassification Error (%)      5.329949\n",
      "3                   Loss Error      0.184656\n"
     ]
    }
   ],
   "source": [
    "res = bert.predict(test,\n",
    "                   text_parms=bert.get_text_parameters())\n",
    "print(res['ScoreInfo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
